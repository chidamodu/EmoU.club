{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hidden Markov Model - to attach probabilities to words and tags\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "\n",
    "def reading_file(document): \n",
    "    filtered_sentence=[]\n",
    "    res = defaultdict(list)\n",
    "    df=pd.read_csv(document, names=['City', 'Details'], header=None, encoding='latin-1')\n",
    "    df['Details'] = df['Details'].str.replace(',', ' ')\n",
    "    for i, j in df.itertuples(index=False):\n",
    "        s=re.split(r'(?<=\\.) ', j )\n",
    "        tag_sent=[nltk.word_tokenize(i) for i in s]\n",
    "        pos_tag_sent=[nltk.pos_tag(sent) for sent in tag_sent]\n",
    "        res[i].append(pos_tag_sent)\n",
    "    return (res)\n",
    "\n",
    "\n",
    "def hmm_viterbi(doc):\n",
    "    len_tagged_doc=len(doc)\n",
    "    word_tag_prob={}\n",
    "    max_of_tag_for_word={}\n",
    "    \n",
    "    tag_list=[]\n",
    "    word_list=[]\n",
    "    result=[]\n",
    "\n",
    "    for k in doc:#this is the key in the dictionary like New York\n",
    "        for vita in doc[k]:#this is the blob of text - here it's in the form of list of lists containing tuples of words and tags\n",
    "            for line in vita:\n",
    "                result.append(line)\n",
    "                for elem in line:\n",
    "                    word_list.append(elem[0])\n",
    "                    tag_list.append(elem[1])\n",
    "                    \n",
    "    for i in range (len(tag_list)):\n",
    "        if (i+1) < (len(tag_list)):\n",
    "            current_tag=tag_list[i+1]#inner: inner key of the transition_prob dictionary            \n",
    "            previous_tag=tag_list[i]#outer: outer key of the transition_prob dictionary\n",
    "            transition_prob[previous_tag]=transition_prob.get(previous_tag, {})\n",
    "            transition_prob[previous_tag][current_tag]=transition_prob[previous_tag].get(current_tag, 0)\n",
    "            transition_prob[previous_tag][current_tag]+=1\n",
    "        \n",
    "        #setting up the word-tag dictionary to get the word likelihood probability\n",
    "            word_key=word_list[i]#outer: outer key of the word_tag_prob dictionary\n",
    "            tag_key=tag_list[i]#inner: inner key of the word_tag_prob dictionary\n",
    "            word_tag_prob[word_key]=word_tag_prob.get(word_key, {})\n",
    "            word_tag_prob[word_key][tag_key]=word_tag_prob[word_key].get(tag_key, 0)\n",
    "            word_tag_prob[word_key][tag_key]+=1\n",
    "        \n",
    "       #to account for first word after '.'\n",
    "    transition_prob['.'] = transition_prob.get('.',{})\n",
    "    transition_prob['.'][tag_list[0]] = transition_prob['.'].get(tag_list[0],0)\n",
    "    transition_prob['.'][tag_list[0]]+=1\n",
    "        \n",
    "        #last word_tag pair\n",
    "    last_word_tag_index=len_tagged_doc-1        \n",
    "    last_word_key=word_list[last_word_tag_index]#outer key of the word_tag_prob dictionary\n",
    "    last_tag_key=tag_list[last_word_tag_index]#inner key of the word_tag_prob dictionary\n",
    "    word_tag_prob[last_word_key]=word_tag_prob.get(last_word_key, {})\n",
    "    word_tag_prob[last_word_key][last_tag_key]=word_tag_prob[last_word_key].get(last_tag_key, 0)\n",
    "    word_tag_prob[last_word_key][last_tag_key]+=1\n",
    "\n",
    "    #calculating probabilities using the count-values from the dictionaries and assigning them back to the dictionary.so now we have probabilities for tags dictionary instead of counts  \n",
    "    for k in transition_prob:\n",
    "        ans=transition_prob[k]\n",
    "        s=sum(ans.values())\n",
    "        for j in ans:\n",
    "            ans[j]= round(ans[j]/s, 1)\n",
    "        ans=ans.items()\n",
    "        ans=sorted(ans,key=lambda x: x[0])\n",
    "        transition_prob[k]=ans\n",
    "\n",
    "    \n",
    "    #calculating probabilities for word-tag dictionary (it's just a dictionary with words and their associated tags. here i find probability of the tags associated with their words. if a word has more than one tag means the word has appeared more than once!)\n",
    "    for key in word_tag_prob:\n",
    "        aw=word_tag_prob[key]\n",
    "        max_of_tag_for_word[key]=max(aw, key=aw.get)\n",
    "        su=sum(aw.values())\n",
    "        for ele in aw:\n",
    "            aw[ele]= round(aw[ele]/su, 1)\n",
    "        aw=aw.items()\n",
    "        aw=sorted(aw, key=lambda x: x[0])\n",
    "        word_tag_prob[key]=aw\n",
    "    \n",
    "    \n",
    "    \n",
    "#     by transition_prob i mean transition from one state(tag) to another and probabilities of the transition  \n",
    "#     max_of_tag_for_word : (meaning getting the tag that has maximum probability and assigning that to the word from every word-tag list)\n",
    "    return( word_tag_prob, transition_prob, word_list, tag_list, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to find the optimal path of a sentence to extract maximum information out of text\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "\n",
    "def viterbi(values_from_hmm):#output from the above function: hmm_viterbi are assigned to the following four variables\n",
    "    my_dict = defaultdict(list)\n",
    "    emission_probability=values_from_hmm[0]\n",
    "    transition_probability=values_from_hmm[1]\n",
    "    observations=values_from_hmm[2]#words\n",
    "    states=values_from_hmm[3]#tags\n",
    "    word_tag_from_doc=values_from_hmm[4]\n",
    "    \n",
    "    new_path = []\n",
    "    already_path=[]\n",
    "    prob_previous=0\n",
    "    previous_tag=None\n",
    "    result_newpath=[]\n",
    "    \n",
    " \n",
    "    \n",
    "    # Add the probabilities of beginning the sequence with each possible state  \n",
    "        \n",
    "    for e in word_tag_from_doc:#i use this to have access to the list of lists\n",
    "        already_path.append(e)\n",
    "        for (a, b) in e:\n",
    "            if a == e[0][0]:#meaning if a is the first word of every list\n",
    "                if a in emission_probability:#i use this because this is the one that's calculated by using HMM\n",
    "                    for itc in emission_probability[a]:\n",
    "                        present_tag=itc[0]#corresponding current tag according to hmm word_tag directory\n",
    "                        present_val=itc[1]#corresponding current tag value of the word that i am looping through    \n",
    "                        max_cal=[]\n",
    "                        max_cal.append(((present_val*1), present_tag))\n",
    "                    anw = sorted(max_cal, key=lambda x: x, reverse=True)[0]#check whether the value was output in a list if yes add [0]                    \n",
    "                    previous_tag=anw[1]\n",
    "                    prob_previous=anw[0]\n",
    "                    new_path.append([])\n",
    "                    i = len(new_path)-1\n",
    "                    new_path[i].append((a, previous_tag))\n",
    "                      \n",
    "            else:         \n",
    "                if a in emission_probability:#i use this because this is the one that's calculated by using HMM\n",
    "                    for itc in emission_probability[a]:\n",
    "                        present_tag=itc[0]#corresponding current tag according to hmm word_tag directory\n",
    "                        present_val=itc[1]#corresponding current tag value of the word that i am looping through\n",
    "                        try:\n",
    "                            ans=float([item[1] for item in transition_probability[previous_tag] if present_tag in item][0])\n",
    "                        except IndexError: ## not the best way to code...do not catch exceptions, but this is an exception to the exception catching rule\n",
    "                            ans=0.1\n",
    "                        max_cal=[]\n",
    "                        max_cal.append(((present_val*prob_previous*ans), present_tag))#sample\n",
    "                    \n",
    "                    anw = sorted(max_cal, key=lambda x: x, reverse=True)[0]#check whether the value was output in a list if yes add [0]                    \n",
    "                    previous_tag=anw[1]\n",
    "                    prob_previous=anw[0]                        \n",
    "                    new_path[i].append((a, previous_tag))\n",
    "            \n",
    "    return(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extracting information/relationship-using nltk.ne_chunk\n",
    "\n",
    "from nltk.sem.relextract import tree2semi_rel, semi_rel2reldict\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "document=viterbi(hmm_viterbi(reading_file('/Users/chidam/Desktop/Barcelona.csv')))\n",
    "chunk_result=[]\n",
    "for i in document:\n",
    "    res=nltk.ne_chunk(i)\n",
    "    chunk_result.append(semi_rel2reldict(tree2semi_rel(res)))\n",
    "result=[]\n",
    "for veg in chunk_result:\n",
    "    for carrot in veg:\n",
    "        result.append([])\n",
    "        i = len(result)-1\n",
    "        result[i].append((carrot['untagged_filler']))\n",
    "result\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
